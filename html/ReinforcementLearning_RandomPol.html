
<!DOCTYPE html
  PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html><head>
      <meta http-equiv="Content-Type" content="text/html; charset=utf-8">
   <!--
This HTML was auto-generated from MATLAB code.
To make changes, update the MATLAB code and republish this document.
      --><title>ReinforcementLearning_RandomPol</title><meta name="generator" content="MATLAB 8.3"><link rel="schema.DC" href="http://purl.org/dc/elements/1.1/"><meta name="DC.date" content="2015-02-21"><meta name="DC.source" content="ReinforcementLearning_RandomPol.m"><style type="text/css">
html,body,div,span,applet,object,iframe,h1,h2,h3,h4,h5,h6,p,blockquote,pre,a,abbr,acronym,address,big,cite,code,del,dfn,em,font,img,ins,kbd,q,s,samp,small,strike,strong,sub,sup,tt,var,b,u,i,center,dl,dt,dd,ol,ul,li,fieldset,form,label,legend,table,caption,tbody,tfoot,thead,tr,th,td{margin:0;padding:0;border:0;outline:0;font-size:100%;vertical-align:baseline;background:transparent}body{line-height:1}ol,ul{list-style:none}blockquote,q{quotes:none}blockquote:before,blockquote:after,q:before,q:after{content:'';content:none}:focus{outine:0}ins{text-decoration:none}del{text-decoration:line-through}table{border-collapse:collapse;border-spacing:0}

html { min-height:100%; margin-bottom:1px; }
html body { height:100%; margin:0px; font-family:Arial, Helvetica, sans-serif; font-size:10px; color:#000; line-height:140%; background:#fff none; overflow-y:scroll; }
html body td { vertical-align:top; text-align:left; }

h1 { padding:0px; margin:0px 0px 25px; font-family:Arial, Helvetica, sans-serif; font-size:1.5em; color:#d55000; line-height:100%; font-weight:normal; }
h2 { padding:0px; margin:0px 0px 8px; font-family:Arial, Helvetica, sans-serif; font-size:1.2em; color:#000; font-weight:bold; line-height:140%; border-bottom:1px solid #d6d4d4; display:block; }
h3 { padding:0px; margin:0px 0px 5px; font-family:Arial, Helvetica, sans-serif; font-size:1.1em; color:#000; font-weight:bold; line-height:140%; }

a { color:#005fce; text-decoration:none; }
a:hover { color:#005fce; text-decoration:underline; }
a:visited { color:#004aa0; text-decoration:none; }

p { padding:0px; margin:0px 0px 20px; }
img { padding:0px; margin:0px 0px 20px; border:none; }
p img, pre img, tt img, li img, h1 img, h2 img { margin-bottom:0px; } 

ul { padding:0px; margin:0px 0px 20px 23px; list-style:square; }
ul li { padding:0px; margin:0px 0px 7px 0px; }
ul li ul { padding:5px 0px 0px; margin:0px 0px 7px 23px; }
ul li ol li { list-style:decimal; }
ol { padding:0px; margin:0px 0px 20px 0px; list-style:decimal; }
ol li { padding:0px; margin:0px 0px 7px 23px; list-style-type:decimal; }
ol li ol { padding:5px 0px 0px; margin:0px 0px 7px 0px; }
ol li ol li { list-style-type:lower-alpha; }
ol li ul { padding-top:7px; }
ol li ul li { list-style:square; }

.content { font-size:1.2em; line-height:140%; padding: 20px; }

pre, code { font-size:12px; }
tt { font-size: 1.2em; }
pre { margin:0px 0px 20px; }
pre.codeinput { padding:10px; border:1px solid #d3d3d3; background:#f7f7f7; }
pre.codeoutput { padding:10px 11px; margin:0px 0px 20px; color:#4c4c4c; }
pre.error { color:red; }

@media print { pre.codeinput, pre.codeoutput { word-wrap:break-word; width:100%; } }

span.keyword { color:#0000FF }
span.comment { color:#228B22 }
span.string { color:#A020F0 }
span.untermstring { color:#B20000 }
span.syscmd { color:#B28C00 }

.footer { width:auto; padding:10px 0px; margin:25px 0px 0px; border-top:1px dotted #878787; font-size:0.8em; line-height:140%; font-style:italic; color:#878787; text-align:left; float:none; }
.footer p { margin:0px; }
.footer a { color:#878787; }
.footer a:hover { color:#878787; text-decoration:underline; }
.footer a:visited { color:#878787; }

table th { padding:7px 5px; text-align:left; vertical-align:middle; border: 1px solid #d6d4d4; font-weight:bold; }
table td { padding:7px 5px; text-align:left; vertical-align:top; border:1px solid #d6d4d4; }





  </style></head><body><div class="content"><pre class="codeinput"><span class="keyword">function</span> q=ReinforcementLearning_RandomPol(R, gamma, goalState, alpha)
<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
<span class="comment">% Original Q Learning by Example code, by Kardi Teknomo</span>
<span class="comment">% (http://people.revoledu.com/kardi/)</span>
<span class="comment">%</span>
<span class="comment">% Code amended by Ioannis Makris and Andrew Chalikiopoulos</span>
<span class="comment">% Model for an agent to find shortest path through a 5x5 maze grid</span>
<span class="comment">% This algorithm uses a random policy to choose the next state</span>
<span class="comment">%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%</span>
clc;
format <span class="string">short</span>
format <span class="string">compact</span>

<span class="comment">% Three inputs: R, gamma and alpha</span>
<span class="keyword">if</span> nargin&lt;1,
<span class="comment">% immediate reward matrix</span>
    R=RewardMatrix25;
<span class="keyword">end</span>
<span class="keyword">if</span> nargin&lt;2,
    gamma=0.80;              <span class="comment">% discount factor</span>
    alpha=0.80;              <span class="comment">% learning rate</span>
<span class="keyword">end</span>
<span class="keyword">if</span> nargin&lt;3
    goalState=22;
<span class="keyword">end</span>

q=zeros(size(R));        <span class="comment">% initialize Q as zero</span>
q1=ones(size(R))*inf;    <span class="comment">% initialize previous Q as big number</span>
count=0;                 <span class="comment">% counter</span>
steps=0;                 <span class="comment">% counts the number of steps to goal</span>
episodes=0;              <span class="comment">% counts the number of episodes</span>
B=[];                    <span class="comment">% matrix to add results of steps and episode count</span>
cumReward=0;             <span class="comment">% counter to calculate accumulated reward</span>



<span class="keyword">for</span> episode=0:50000 <span class="comment">% the amount of episodes to run</span>

    state=5;        <span class="comment">% Starting state of the agent</span>


    <span class="keyword">while</span> state~=goalState            <span class="comment">% loop until find goal state</span>
        <span class="comment">% select any action from this state</span>
        x=find(R(state,:)&gt;=0)         <span class="comment">% find possible action of this state</span>
        <span class="keyword">if</span> size(x,1)&gt;0,
            x1=RandomPermutation(x);   <span class="comment">% randomize the possible action</span>
            x1=x1(1);                  <span class="comment">% select an action (only the first element of random sequence)</span>
            cumReward=cumReward+q(state,x1);
        <span class="keyword">end</span>

        x2 = find(R(x1,:)&gt;=0);   <span class="comment">% find possible steps from next step</span>
        qMax=(max(q(x1,x2(1:end)))); <span class="comment">% extract qmax from all possible next states</span>
        q(state,x1)= q(state,x1)+alpha*((R(state,x1)+gamma*qMax)-q(state,x1))    <span class="comment">% Temporal Difference Error</span>
        state=x1;    <span class="comment">% set state to next state</span>

        <span class="keyword">if</span> state~=goalState     <span class="comment">% keep track of steps taken if goal not reached</span>
            steps=steps+1;

        <span class="keyword">else</span>
            episodes=episodes+1; <span class="comment">% if goal reach increase episode counter</span>
            A=[episodes; steps; cumReward;];   <span class="comment">% create episodes, steps and cumReward matrix</span>
            B=horzcat(B, A);    <span class="comment">% add the new results to combined matrix</span>
            steps=0;    <span class="comment">% reset steps counter to 0</span>
            cumReward=0;    <span class="comment">% reset cumReward counter to</span>

        <span class="keyword">end</span>

    <span class="keyword">end</span>


     <span class="comment">% break if convergence: small deviation on q for 1000 consecutive</span>
     <span class="keyword">if</span> sum(sum(abs(q1-q)))&lt;0.00001 &amp;&amp; sum(sum(q &gt;0))
         <span class="keyword">if</span> count&gt;1000,
             episode  <span class="comment">% report last episode</span>
             <span class="keyword">break</span> <span class="comment">% for loop</span>
         <span class="keyword">else</span>
             count=count+1; <span class="comment">% set counter if deviation of q is small</span>
         <span class="keyword">end</span>
     <span class="keyword">else</span>
         q1=q
         count=0;  <span class="comment">% reset counter when deviation of q from previous q is large</span>
     <span class="keyword">end</span>
<span class="keyword">end</span>

<span class="comment">% row 4 in matrix is cumReward/steps taken per episode</span>
B(4,:) = (B(3,:)./B(2,:));

<span class="comment">%episodes vs cumReward taken averaged against steps taken</span>
<span class="comment">%plot(B(1,:),B(4,:));</span>

<span class="comment">% create a plot of episodes vs steps taken and episodes vs cumReward taken averaged against steps taken</span>
figure <span class="comment">% new figure</span>
[hAx] = plotyy(B(1,1 : 5 : end),B(2,1 : 5 : end), B(1,1 : 5 : end),B(4,1 : 5 : end));

title(<span class="string">'Q-Learning Performance'</span>)
xlabel(<span class="string">'Episodes'</span>)
ylabel(hAx(1),<span class="string">'Steps'</span>) <span class="comment">% left y-axis</span>
ylabel(hAx(2),<span class="string">'Cumulative Reward/Steps'</span>) <span class="comment">% right y-axis</span>

<span class="comment">% create a plot of episodes vs cumReward</span>
<span class="comment">%plot(B(1,1 : 5 : end),B(3,1 : 5 : end));</span>

<span class="comment">%normalize q</span>
g=max(max(q));
<span class="keyword">if</span> g&gt;0,
    q=100*q/g;
<span class="keyword">end</span>

<span class="comment">% display the shortest path to the goal</span>
  Optimal=[];
  state=5;
  Optimal=horzcat(Optimal,state);

  <span class="keyword">while</span> state~=goalState

           [~,optimal]=(max(q(state,:)));
           state = optimal;
           Optimal=horzcat(Optimal,state);
  <span class="keyword">end</span>
  display(<span class="string">'Shortest path:'</span>)
  display(Optimal);
</pre><p class="footer"><br><a href="http://www.mathworks.com/products/matlab/">Published with MATLAB&reg; R2014a</a><br></p></div><!--
##### SOURCE BEGIN #####
function q=ReinforcementLearning_RandomPol(R, gamma, goalState, alpha)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Original Q Learning by Example code, by Kardi Teknomo 
% (http://people.revoledu.com/kardi/)
%
% Code amended by Ioannis Makris and Andrew Chalikiopoulos
% Model for an agent to find shortest path through a 5x5 maze grid
% This algorithm uses a random policy to choose the next state
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%  
clc;
format short
format compact

% Three inputs: R, gamma and alpha
if nargin<1,
% immediate reward matrix
    R=RewardMatrix25;
end
if nargin<2,
    gamma=0.80;              % discount factor
    alpha=0.80;              % learning rate
end
if nargin<3
    goalState=22;
end

q=zeros(size(R));        % initialize Q as zero
q1=ones(size(R))*inf;    % initialize previous Q as big number
count=0;                 % counter
steps=0;                 % counts the number of steps to goal
episodes=0;              % counts the number of episodes
B=[];                    % matrix to add results of steps and episode count
cumReward=0;             % counter to calculate accumulated reward



for episode=0:50000 % the amount of episodes to run

    state=5;        % Starting state of the agent
   
    
    while state~=goalState            % loop until find goal state
        % select any action from this state
        x=find(R(state,:)>=0)         % find possible action of this state
        if size(x,1)>0,
            x1=RandomPermutation(x);   % randomize the possible action
            x1=x1(1);                  % select an action (only the first element of random sequence)
            cumReward=cumReward+q(state,x1);
        end
        
        x2 = find(R(x1,:)>=0);   % find possible steps from next step
        qMax=(max(q(x1,x2(1:end)))); % extract qmax from all possible next states
        q(state,x1)= q(state,x1)+alpha*((R(state,x1)+gamma*qMax)-q(state,x1))    % Temporal Difference Error
        state=x1;    % set state to next state
        
        if state~=goalState     % keep track of steps taken if goal not reached
            steps=steps+1;
            
        else
            episodes=episodes+1; % if goal reach increase episode counter
            A=[episodes; steps; cumReward;];   % create episodes, steps and cumReward matrix
            B=horzcat(B, A);    % add the new results to combined matrix
            steps=0;    % reset steps counter to 0
            cumReward=0;    % reset cumReward counter to 
            
        end

    end
    
        
     % break if convergence: small deviation on q for 1000 consecutive
     if sum(sum(abs(q1-q)))<0.00001 && sum(sum(q >0))
         if count>1000,
             episode  % report last episode
             break % for loop
         else
             count=count+1; % set counter if deviation of q is small
         end
     else
         q1=q
         count=0;  % reset counter when deviation of q from previous q is large
     end
end   

% row 4 in matrix is cumReward/steps taken per episode
B(4,:) = (B(3,:)./B(2,:));

%episodes vs cumReward taken averaged against steps taken
%plot(B(1,:),B(4,:));

% create a plot of episodes vs steps taken and episodes vs cumReward taken averaged against steps taken
figure % new figure
[hAx] = plotyy(B(1,1 : 5 : end),B(2,1 : 5 : end), B(1,1 : 5 : end),B(4,1 : 5 : end));

title('Q-Learning Performance')
xlabel('Episodes')
ylabel(hAx(1),'Steps') % left y-axis
ylabel(hAx(2),'Cumulative Reward/Steps') % right y-axis

% create a plot of episodes vs cumReward
%plot(B(1,1 : 5 : end),B(3,1 : 5 : end));

%normalize q
g=max(max(q));
if g>0, 
    q=100*q/g;
end

% display the shortest path to the goal
  Optimal=[];
  state=5;
  Optimal=horzcat(Optimal,state);
 
  while state~=goalState
      
           [~,optimal]=(max(q(state,:)));
           state = optimal;
           Optimal=horzcat(Optimal,state);         
  end
  display('Shortest path:')
  display(Optimal);
##### SOURCE END #####
--></body></html>